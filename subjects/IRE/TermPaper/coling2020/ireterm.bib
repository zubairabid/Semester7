
@book{geser_digicult_2002,
	title = {The {DigiCULT} report: technological landscapes for tomorrow's cultural economy: unlocking the value of cultural heritage: executive summary: {January} 2002},
	publisher = {Office for official publications of the European Communities},
	author = {Geser, Guntram and Mulrenin, Andrea},
	year = {2002}
}

@misc{noauthor_internet_2016,
	title = {Internet access, local content must for {Digital} {India} success: {IAMAI} - {The} {Economic} {Times}},
	url = {https://economictimes.indiatimes.com/tech/internet/internet-access-local-content-must-for-digital-india-success-iamai/articleshow/45270913.cms},
	urldate = {2020-11-10},
	year = {2016},
	file = {Internet access, local content must for Digital India success\: IAMAI - The Economic Times:/home/zubair/Zotero/storage/H6RWH2WW/45270913.html:text/html}
}

@book{cummins_bilingualism_1981,
	title = {Bilingualism and {Minority}-{Language} {Children}. {Language} and {Literacy} {Series}},
	isbn = {978-0-7744-0239-2},
	url = {https://eric.ed.gov/?id=ED215557},
	abstract = {This handbook provides an introduction to research findings related to bilingualism in minority-language children, and describes the implications of these findings for issues of current concern in Canadian education. Bilingualism is defined as the production and/or comprehension of two languages by the same individual. The phrase "minority-language children" refers to children whose first language is different from the language of the wider community. The topic is discussed under five headings: (1) issues dealing with bilingual and bicultural education, providing for instruction in a variety of languages, psychological and educational ramifications, and a case study; (2) the historical perspective and the context for bilingualism and bilingual education at present in Canada and  in other countries; (3) a presentation of research findings and a consideration of the patterns of bilingualism and cultural identity typically developed by minority children; (4) a review of theories related to learning two languages and a formulation of a cognitive "think tank model" for language learning; and (5) a consideration of the practical implications of the research findings for "heritage-language" teachers and minority parents who are eager to promote a high level of first language proficiency. The book concludes with a summary of what is known about bilingualism and children's development. (AMH)},
	language = {en},
	urldate = {2020-11-10},
	publisher = {The Ontario Institute for Studies in Education, 252 Bloor Street West, Toronto, Ontario M5S 1V6 (\$3},
	author = {Cummins, Jim},
	year = {1981},
	keywords = {Bilingual Education, Bilingualism, Child Language, Children, Cultural Background, Language Maintenance, Language Processing, Language Research, Minority Groups, Second Language Learning},
	file = {Full Text PDF:/home/zubair/Zotero/storage/TJL8C5DY/Cummins - 1981 - Bilingualism and Minority-Language Children. Langu.pdf:application/pdf;Snapshot:/home/zubair/Zotero/storage/7JQTV8JD/eric.ed.gov.html:text/html}
}

@misc{noauthor_knowledge_2020,
	title = {Knowledge and {Language}},
	url = {https://tok2022.weebly.com/knowledge-and-language.html},
	abstract = {Language is a medium through which we pass on most knowledge. You could ask yourself how much you would know if you had no language to gather or express knowledge. Our daily language is heavily...},
	language = {en},
	urldate = {2020-11-10},
	journal = {TOK 2022: THEORY OF KNOWLEDGE WEBSITE FOR THE IBDP},
	year = {2020},
	file = {Snapshot:/home/zubair/Zotero/storage/W267ASTB/knowledge-and-language.html:text/html}
}

@book{hakuta_compendium_1986,
	title = {Compendium of {Papers} on the {Topic} of {Bilingual} {Education} of the {Committee} on {Education} and {Labor}, {House} of {Representatives}, 99th {Congress}, 2nd {Session}},
	language = {en},
	publisher = {U.S. Government Printing Office},
	author = {Hakuta, K and Snow, C},
	year = {1986},
	note = {Google-Books-ID: yboZAAAAMAAJ}
}

@article{hudelson_role_1987,
	title = {The {Role} of {Native} {Language} {Literacy} in the {Education} of {Language} {Minority} {Children}},
	volume = {64},
	issn = {0360-9170},
	url = {https://www.jstor.org/stable/41961686},
	number = {8},
	urldate = {2020-11-10},
	journal = {Language Arts},
	author = {Hudelson, Sarah},
	year = {1987},
	note = {Publisher: National Council of Teachers of English},
	pages = {827--841}
}

@misc{canales_for_2020,
	title = {For years, an {American} has been writing articles in a stereotypical {Scottish} accent on the official {Scots} {Wikipedia}, and some people online are not happy},
	url = {https://www.businessinsider.in/tech/news/for-years-an-american-has-been-writing-articles-in-a-stereotypical-scottish-accent-on-the-official-scots-wikipedia-and-some-people-online-are-not-happy/articleshow/77792317.cms},
	abstract = {An American has been writing and editing thousands of articles on the official Wikipedia page for the Scots language, according to Motherboard. AmaryllisGardener,},
	urldate = {2020-11-10},
	journal = {Business Insider},
	author = {Canales, Katie},
	year = {2020},
	file = {Snapshot:/home/zubair/Zotero/storage/BIND63WQ/77792317.html:text/html}
}

@misc{ethnologue_english_2019,
	title = {English},
	url = {https://www.ethnologue.com/language/eng},
	abstract = {A language profile for English. Get a detailed look at the language, from population to dialects and usage.},
	language = {en},
	urldate = {2020-11-10},
	journal = {Ethnologue},
	author = {Ethnologue},
	year = {2019},
	file = {Snapshot:/home/zubair/Zotero/storage/68ZMBFP5/eng.html:text/html}
}

@misc{ethnologue_what_2019,
	title = {What are the top 200 most spoken languages?},
	url = {https://www.ethnologue.com/guides/ethnologue200},
	abstract = {View the largest 200 languages: over 88\% of people speak one of these languages as their native tongue.},
	language = {en},
	urldate = {2020-11-10},
	journal = {Ethnologue},
	author = {Ethnologue},
	year = {2019},
	file = {Snapshot:/home/zubair/Zotero/storage/ICEK53YX/ethnologue200.html:text/html}
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	pages = {24},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/home/zubair/Zotero/storage/FTD8B63T/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf}
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2020-11-10},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zubair/Zotero/storage/T2QBELK2/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/zubair/Zotero/storage/7YRKAPE2/2005.html:text/html}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-11-10},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zubair/Zotero/storage/QGXMZ46B/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/home/zubair/Zotero/storage/MKVBZ8UL/1810.html:text/html}
}

@misc{initiative_wai_w3c_nodate,
	title = {{W3C} {Accessibility} {Standards} {Overview}},
	url = {https://www.w3.org/WAI/standards-guidelines/},
	abstract = {The Website of the World Wide Web Consortium’s Web Accessibility Initiative.},
	language = {en},
	urldate = {2020-11-10},
	journal = {Web Accessibility Initiative (WAI)},
	author = {Initiative (WAI), W3C Web Accessibility},
	file = {Snapshot:/home/zubair/Zotero/storage/9KXMINAC/standards-guidelines.html:text/html}
}
